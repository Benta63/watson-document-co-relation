{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Correlation of text content across documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "To prepare your environment, you need to install some packages and enter credentials for the Watson services.\n",
    "\n",
    "### 1.1 Install the necessary packages\n",
    "\n",
    "You need the latest versions of these packages:<br>\n",
    "Watson Developer Cloud: a client library for Watson services.<br>\n",
    "NLTK: leading platform for building Python programs to work with human language data.<br>\n",
    "python-keystoneclient: is a client for the OpenStack Identity API.<br>\n",
    "python-swiftclient: is a python client for the Swift API.<br><br>\n",
    "** Install the Watson Developer Cloud package: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade watson-developer-cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Install NLTK: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Install IBM Bluemix Object Storage Client: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install python-keystoneclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-swiftclient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** <font color=blue>Now restart the kernel by choosing Kernel > Restart. </font> **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Import packages and libraries\n",
    "\n",
    "Import the packages and libraries that you'll use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import thread\n",
    "import time\n",
    "import watson_developer_cloud\n",
    "from watson_developer_cloud import NaturalLanguageUnderstandingV1\n",
    "import watson_developer_cloud.natural_language_understanding.features.v1 \\\n",
    "  as Features\n",
    "    \n",
    "import swiftclient\n",
    "from keystoneclient import client\n",
    "    \n",
    "import operator\n",
    "from functools import reduce\n",
    "from io import StringIO\n",
    "import numpy as np\n",
    "from os.path import join, dirname\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import word_tokenize,sent_tokenize,ne_chunk\n",
    "from nltk.corpus import stopwords\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Add configurable items of the notebook below\n",
    "\n",
    "### 2.1 Add your service credentials from Bluemix for the Watson services\n",
    "\n",
    "You must create a Watson Natural Language Understanding service on Bluemix.\n",
    "Create a service for Natural Language Understanding (NLU).\n",
    "Insert the username and password values for your NLU in the following cell. Do not change the values of the version fields.\n",
    "\n",
    "Run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# @hidden_cell\n",
    "natural_language_understanding = NaturalLanguageUnderstandingV1(\n",
    "    version='',\n",
    "    username=\"\",\n",
    "    password=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Add your service credentials for Object Storage\n",
    "\n",
    "You must create Object Storage service on Bluemix.\n",
    "To access data in a file in Object Storage, you need the Object Storage authentication credentials.\n",
    "Insert the Object Storage authentication credentials in the following cell. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# @hidden_cell\n",
    "credentials_1 = {\n",
    "  'auth_url':'',\n",
    "  'project':'',\n",
    "  'project_id':'',\n",
    "  'region':'',\n",
    "  'user_id':'',\n",
    "  'domain_id':'',\n",
    "  'domain_name':'',\n",
    "  'username':'',\n",
    "  'password':\"\",\n",
    "  'container':'',\n",
    "  'tenantId':'',\n",
    "  'filename':''\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Global Variables\n",
    "\n",
    "Add global variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Specify file names for sample text and configuration files\n",
    "sampleTextFileName1 = \"sample_text_1.txt\"\n",
    "sampleTextFileName2 = \"sample_text_2.txt\"\n",
    "sampleConfigFileName = \"sample_config.txt\"\n",
    "\n",
    "# Maintain tagged text and plain text map\n",
    "tagTextMap ={}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Configure and download required NLTK packages\n",
    "\n",
    "Download the 'punkt' and 'averaged_perceptron_tagger' NLTK packages for POS tagging usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Correlation\n",
    "\n",
    "Write the correlation related utility functions in a modularalized form.\n",
    "\n",
    "### 3.1 Watson NLU API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def analyze_using_NLU(analysistext):\n",
    "    response = natural_language_understanding.analyze( \n",
    "        text=analysistext,features=[ Features.Entities(\n",
    "                                        emotion=True,\n",
    "                                        sentiment=True\n",
    "                                     ),\n",
    "                                     Features.Keywords(\n",
    "                                        emotion=True,\n",
    "                                        sentiment=True\n",
    "                                     ),\n",
    "                                     Features.SemanticRoles(\n",
    "                                        entities=True,\n",
    "                                        keywords=True\n",
    "                                     )                                   \n",
    "                                   ] )\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Augumented Correlation\n",
    "\n",
    "Custom correlation utlity fucntions for augumenting the results of Watson NLU API call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentences(text):\n",
    "    \"\"\" Split text into sentences.\n",
    "    \"\"\"\n",
    "    sentence_delimiters = re.compile(u'[\\\\[\\\\]\\n.!?]')\n",
    "    sentences = sentence_delimiters.split(text)\n",
    "    return sentences\n",
    "\n",
    "def split_into_tokens(text):\n",
    "    \"\"\" Split text into tokens.\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens\n",
    "    \n",
    "def POS_tagging(text):\n",
    "    \"\"\" Generate Part of speech tagging of the text.\n",
    "    \"\"\"\n",
    "    POSofText = nltk.tag.pos_tag(text)\n",
    "    return POSofText\n",
    "\n",
    "def keyword_tagging(tag,tagtext,text):\n",
    "    \"\"\" Tag the text matching keywords.\n",
    "    \"\"\"\n",
    "    if (text.lower().find(tagtext.lower()) != -1):\n",
    "        return text[text.lower().find(tagtext.lower()):text.lower().find(tagtext.lower())+len(tagtext)]\n",
    "    else:\n",
    "        return 'UNKNOWN'\n",
    "    \n",
    "def regex_tagging(tag,regex,text):\n",
    "    \"\"\" Tag the text matching REGEX.\n",
    "    \"\"\"    \n",
    "    p = re.compile(regex, re.IGNORECASE)\n",
    "    matchtext = p.findall(text)\n",
    "    regex_list=[]    \n",
    "    if (len(matchtext)>0):\n",
    "        for regword in matchtext:\n",
    "            regex_list.append(regword)\n",
    "    return regex_list\n",
    "\n",
    "def chunk_tagging(tag,chunk,text):\n",
    "    \"\"\" Tag the text using chunking.\n",
    "    \"\"\"\n",
    "    parsed_cp = nltk.RegexpParser(chunk)\n",
    "    pos_cp = parsed_cp.parse(text)\n",
    "    chunk_list=[]\n",
    "    for root in pos_cp:\n",
    "        if isinstance(root, nltk.tree.Tree):               \n",
    "            if root.label() == tag:\n",
    "                chunk_word = ''\n",
    "                for child_root in root:\n",
    "                    chunk_word = chunk_word +' '+ child_root[0]\n",
    "                chunk_list.append(chunk_word)\n",
    "    return chunk_list\n",
    "    \n",
    "def augument_NLUResponse(responsejson,updateType,text,tag):\n",
    "    \"\"\" Update the NLU response JSON with augumented classifications.\n",
    "    \"\"\"\n",
    "    if(updateType == 'keyword'):\n",
    "        if not any(d.get('text', None) == text for d in responsejson['keywords']):\n",
    "            responsejson['keywords'].append({\"text\":text,\"relevance\":0.5})\n",
    "    else:\n",
    "        if not any(d.get('text', None) == text for d in responsejson['entities']):\n",
    "            responsejson['entities'].append({\"type\":tag,\"text\":text,\"relevance\":0.5,\"count\":1})        \n",
    "    \n",
    "def chunk_sentence(text):\n",
    "    \"\"\" Tag the sentence using chunking.\n",
    "    \"\"\"\n",
    "    grammar = \"\"\"\n",
    "      NP: {<DT|JJ|PRP|NN.*>+} # Chunk sequences of DT,JJ,NN\n",
    "          #}<VB*|DT|JJ|RB|PRP><NN.*>+{  # Chink sequences of VB,DT,JJ,NN       \n",
    "      PP: {<IN><NP>}               # Chunk prepositions followed by NP\n",
    "      V: {<V.*>}                   # Verb      \n",
    "      VP: {<VB*><NP|PP|CLAUSE>+}  # Chunk verbs and their arguments\n",
    "      CLAUSE: {<NP><VP>}           # Chunk NP, VP\n",
    "      \"\"\"  \n",
    "    parsed_cp = nltk.RegexpParser(grammar,loop=2)\n",
    "    pos_cp = parsed_cp.parse(text)\n",
    "    return pos_cp\n",
    "    \n",
    "def find_attrs(subtree,phrase):\n",
    "    attrs = ''\n",
    "    if phrase == 'NP':\n",
    "        for nodes in subtree:\n",
    "            if nodes[1] in ['DT','PRP$','POS','JJ','CD','ADJP','QP','NP','NNP']:\n",
    "                attrs = attrs+' '+nodes[0]\n",
    "    return attrs    \n",
    "    \n",
    "def find_subject(t):\n",
    "    for s in t.subtrees(lambda t: t.label() == 'NP'):\n",
    "        return find_attrs(s,'NP')\n",
    "    \n",
    "def resolve_coreference(text, config):\n",
    "    \"\"\" Resolve coreferences in the text for Nouns that are Subjects in a sentence\n",
    "    \"\"\"\n",
    "    sentenceList = split_sentences(text)\n",
    "    referenceSubject = ''\n",
    "    sentenceText = ''\n",
    "    configjson = json.loads(config)\n",
    "    \n",
    "    for sentences in sentenceList:    \n",
    "        tokens = split_into_tokens(sentences)   \n",
    "        postags = POS_tagging(tokens)\n",
    "        sentencetags = chunk_sentence(postags)\n",
    "        subjects = find_subject(sentencetags)\n",
    "        for rules in configjson['configuration']['coreference']['rules']:\n",
    "            if (rules['type'] == 'chunking'):\n",
    "                for tags in rules['chunk']:\n",
    "                    chunktags = chunk_tagging(tags['tag'],tags['pattern'],postags)\n",
    "                    if (len(chunktags)>0):\n",
    "                        for words in chunktags:\n",
    "                            if tags['tag'] == 'PRP':\n",
    "                                if subjects == '':\n",
    "                                    sentenceText = sentenceText+sentences.replace(words,referenceSubject)+'. '\n",
    "                            elif tags['tag'] == 'NAME':\n",
    "                                if words == subjects:\n",
    "                                    referenceSubject = words\n",
    "                                    sentenceText = sentenceText+sentences+'. '\n",
    "                    \n",
    "    return sentenceText\n",
    "\n",
    "def disambiguate_entities(text):\n",
    "    \"\"\" Resolve disambiguity in the text using entities and entity resolution performed using Watson NLU\n",
    "    \"\"\"    \n",
    "    sentenceList = split_sentences(text)\n",
    "    taggedtext = text\n",
    "    response = analyze_using_NLU(text)\n",
    "    responsejson = response\n",
    "    for sentences in sentenceList:\n",
    "        tokens = split_into_tokens(sentences)\n",
    "        postags = POS_tagging(tokens)\n",
    "        name_tagged_text = chunk_tagging('NAME','NAME:{<NNP>+}',postags)\n",
    "        print(name_tagged_text)\n",
    "    for entities in responsejson['entities']:\n",
    "        regexstr = entities['text']+'(?!>)'\n",
    "        regex = re.compile(regexstr, re.IGNORECASE)\n",
    "        tagText = '<'+entities['type']+':'+entities['text']+'>'\n",
    "        taggedtext = re.sub(regexstr,tagText,taggedtext)\n",
    "        tagTextMap[tagText] = entities['text']\n",
    "    \n",
    "    for roles in responsejson['semantic_roles']:\n",
    "        if 'entities' not in roles['subject']:\n",
    "            print('NO ENTITY')\n",
    "        else:\n",
    "            for entity in roles['subject']['entities']:\n",
    "                if 'disambiguation' not in entity:\n",
    "                    print('NO DISAMBIGUATION')\n",
    "                else:\n",
    "                    regexstr = roles['subject']['text']+'(?!>)'\n",
    "                    regex = re.compile(regexstr, re.IGNORECASE)\n",
    "                    tagText = '<'+entity['type']+':'+entity['text']+'>'\n",
    "                    taggedtext = re.sub(regexstr,tagText,taggedtext)\n",
    "                    tagTextMap[tagText] = entity['text']\n",
    "    \n",
    "    return taggedtext\n",
    "\n",
    "def extract_relations(text, config,relations):\n",
    "    \"\"\" Extract entity relationships in a sentence\n",
    "    \"\"\"    \n",
    "    sentenceList = split_sentences(text)\n",
    "    configjson = json.loads(config)\n",
    "     \n",
    "    for sentences in sentenceList:\n",
    "        for rules in configjson['configuration']['relations']['rules']:\n",
    "            if (rules['type'] == 'd_regex'):\n",
    "                for regex in rules['d_regex']:\n",
    "                    regextags = regex_tagging(regex['tag'],regex['pattern'],sentences)\n",
    "                    if (len(regextags)>0):\n",
    "                        for words in regextags:\n",
    "                            relations.append((tagTextMap[words[0]],regex['tag'],tagTextMap[words[2]]))\n",
    "         \n",
    "    return relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Correlated Network Visualization\n",
    "\n",
    "Utility function to plot network visualization of entity relationships to correlate them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_simple_graph(graph):\n",
    "\n",
    "    nodes = []\n",
    "    labels = []\n",
    "    edges = []\n",
    "    # extract nodes from graph\n",
    "    for tuples in graph:\n",
    "        nodes.append(tuples[0])\n",
    "        nodes.append(tuples[2])\n",
    "        \n",
    "    # extract edges from graph\n",
    "    for edgepairs in graph:\n",
    "        edges.append((edgepairs[0],edgepairs[2]))        \n",
    "    # extract edge labels from graph\n",
    "    for edgetuples in graph:\n",
    "        labels.append(edgetuples[1])\n",
    "    # create networkx graph\n",
    "    G=nx.Graph()\n",
    "    # add nodes\n",
    "    for node in nodes:\n",
    "        G.add_node(node)\n",
    "    # add edges\n",
    "    for edge in graph:\n",
    "        G.add_edge(edge[0], edge[2])\n",
    "\n",
    "    # draw graph\n",
    "    pos = nx.spring_layout(G)\n",
    "    nx.draw(G, pos,with_labels = True)\n",
    "    edge_labels = dict(zip(edges, labels))\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels = edge_labels)\n",
    "\n",
    "    # show graph\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Persistence and Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4.1 Configure Object Storage Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auth_url = credentials_1['auth_url']+\"/v3\"\n",
    "container = credentials_1['container']\n",
    "IBM_Objectstorage_Connection = swiftclient.Connection(\n",
    "    key=credentials_1['password'], authurl=auth_url, auth_version='3', os_options={\n",
    "        \"project_id\": credentials_1['project_id'], \"user_id\": credentials_1['user_id'], \"region_name\": credentials_1['region']})\n",
    "\n",
    "def create_container(container_name):\n",
    "    x = IBM_Objectstorage_Connection.put_container(container_name)\n",
    "    return x\n",
    "\n",
    "def put_object(container_name, fname, contents, content_type):\n",
    "    x = IBM_Objectstorage_Connection.put_object(\n",
    "        container_name,\n",
    "        fname,\n",
    "        contents,\n",
    "        content_type)\n",
    "    return x\n",
    "\n",
    "def get_object(container_name, fname):\n",
    "    Object_Store_file_details = IBM_Objectstorage_Connection.get_object(\n",
    "        container_name, fname)\n",
    "    return Object_Store_file_details[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Correlate text\n",
    "Read the data files for correlation from Object Store<br>\n",
    "Read the configuration file for rules of coreference and correlation from Object Store.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = get_object(container, sampleTextFileName1)\n",
    "text2 = get_object(container, sampleTextFileName2)\n",
    "config = get_object(container, sampleConfigFileName)\n",
    "\n",
    "relationships = []\n",
    "\n",
    "resolved_text1 = resolve_coreference(text1, config)\n",
    "resolved_text2 = resolve_coreference(text2, config)\n",
    "\n",
    "disambiguated_text1 = disambiguate_entities(resolved_text1)\n",
    "disambiguated_text2 = disambiguate_entities(resolved_text2)\n",
    "\n",
    "extract_relations(disambiguated_text1, config,relationships)\n",
    "extract_relations(disambiguated_text2, config,relationships)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize correlated text\n",
    "Load correlated text into Networkx<br>\n",
    "Plot network graph of the correlated text.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "draw_simple_graph(relationships)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2 with Spark 1.6",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
